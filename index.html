<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>AHT</title>
  
  <meta name="author" content="Aaron Hao Tan">
  <meta name="viewport" content="width=device-width, initial-scale=0.85">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">

  <style>
    @media (prefers-color-scheme: dark) {
      body {
        background-color: #121212;
        color: #ffffff;
      }
      
      a {
        color: #64B5F6;
      }
      
      a:hover {
        color: #90CAF9;
      }
      
      name {
        color: #ffffff;
      }
      
      heading {
        color: #ffffff;
      }
      
      papertitle {
        color: #ffffff;
      }
      
      /* Adjust table cell backgrounds if needed */
      td {
        background-color: #121212;
      }
    }
  </style>

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:20px;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Aaron Hao Tan</name>
              <div style="font-size:0.85em; color: #888; text-align:center; margin-top:0.2em; line-height:0.5;">Ë≠öÁöìÂì≤</div>
              </p>

              <p>
                I am currently building robotic lamps at <a href="https://syncereai.github.io">Syncere</a>.
                <br><br>
                Previously, I was a postdoctoral researcher at <a href="https://src.stanford.edu">Stanford</a>,
                
                and completed my PhD at the <a href="https://robotics.utoronto.ca/">University of Toronto</a>. 
                <br><br>
                My research interest includes designing and building robots for human-centric environments, to enable natural human interaction, advanced reasoning, and knowledge sharing among robots.  
              </p>
              

              <!-- <p> -->
                <!-- Previously, I worked on a multi-wheeled mobile robotic system with Dr. <a href="https://scholar.google.ca/citations?user=XOI7ib4AAAAJ&hl=en">Moustafa El-Gindy</a> and vision-based mobile manipulator control with Dr. <a href="https://scholar.google.com/citations?user=GcttJ-oAAAAJ&hl=en">Haoxiang Lang</a>. -->
                <!-- I am an incoming postdoctoral fellow at <a href="https://src.stanford.edu">Stanford</a> (Spring 2025). Previously, I was nominated as a PhD <a href="images/apple scholar nominee.png">Apple Scholar</a> in AI/ML. -->
                <!-- During my time at <a href="https://www.gm.ca/en/home.html">General Motors</a>, I worked on rider sweat prediction for the <a href="https://www.youtube.com/watch?v=WCD9Q_Y4WIA&ab_channel=PedelecsundE-Bikes">ARIV E-Bike</a> project. -->
                
              <!-- </p> -->
              <!-- I am currently building <a href="https://syncereai.github.io">Syncere</a> with <a href="https://angusfung.github.io/">Angus Fung</a> to bring robots in to every household. 
              <br>
              <br> -->
              <!-- If you would like to chat about my work, feel free to book an open slot <a href="https://calendly.com/aaron-stanford/lets-chat">here</a>. -->
              </p>

              <!-- <p>
                Outside of robots, I enjoy keeping myself busy by: <br>
                <b>1)</b> bridging the wealth gap for students via LLM agents at  <a href="https://scholarply.com/">Scholarply</a>,<br>
                <b>2)</b> lowering the barrier of entry to AI via iMessage at  <a href="https://www.instagram.com/one800chat/">ONE800</a>, and<br>
                <b>3)</b> building early disease detection AI tools with Dr. <a href="https://torontoeye.ca/dr-edward-margolin/">Edward Margolin</a>.<br> -->
                <!-- <b>2)</b> building more affordable, and capable 3D printed prosthetics as a robotics and ML consultant at <a href="https://smartarm.ca/">SmartARM</a> and, <br> -->
                <!-- <b>3)</b> collaborating with 2x Grammy Award recipient <a href="https://sidedoormag.com/blog/2022/3/21/sean-leons-success-leading-up-to-being-featured-donda-2">Sean Leon</a> by leading the tech team at Pupil in building AI tools to empower creators globally. -->
              <!-- </p> -->

              <p>
                <em>Updated: 11/25</em>
              </p>

              <p style="text-align:center">
                <a href="mailto:aaronht@stanford.edu">Email</a> &nbsp/&nbsp
                <a href="https://x.com/aaronistan">X</a> &nbsp/&nbsp
                <!-- <a href="files/2024 CV.pdf">CV</a> &nbsp/&nbsp
                <a href="files/resume.pdf">Resume</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=VSW55LkAAAAJ&hl=en">Scholar</a>
                <!-- <a href="https://twitter.com/aaronhaotan">Twitter</a> &nbsp/&nbsp -->
                <!-- <a href="https://github.com/aarontan-git">Github</a> &nbsp/&nbsp <br> -->
                <!-- <a href="https://utoronto-my.sharepoint.com/:f:/g/personal/aaronhao_tan_utoronto_ca/EsclIvvXcN5PseaKFZUfKu8BTeAabLBW7LHbE4VhnVA69A?e=DnyBb2">Shot on iPhone (under development)</a> -->

              </p>
            </td>
            <td style="padding:20px;width:40%;max-width:40%">
              <a href="images/pp_2.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/pp_2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Works</heading>
              <p>
                <!-- I have worked on the entire mobile robotics stack from novel hardware design to perception and control. -->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          

          <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sim2real_image'>
                  <video width=100% height=100% muted autoplay loop playsinline>
                    <source src="images/hamnav.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Mobile Robot Navigation Using Hand-Drawn Maps: A Vision Language Model Approach</papertitle>
              <br>
              <strong>Aaron Hao Tan</strong>,
              <a href="https://angusfung.github.io/">Angus Fung</a>,
              <a href="https://wang-haitong.github.io">Haitong Wang</a>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>IEEE Robotics and Automation Letters 2025 + ICRA 2026</em>
              <br>
							<a href="https://arxiv.org/abs/2502.00114">Paper</a>
              /
							<a href="https://youtu.be/2NOgwqPeIm8?feature=shared">Video</a>
              <p></p>
              <p>We introduce a novel Hand-drawn Map Navigation (HAMNav) architecture that leverages pre-trained vision language models for robot navigation across diverse environments, hand-drawing styles, and robot embodiments, even in the presence of map inaccuracies. </p>
            </td>
          </tr>


          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='sim2real_image'>
                <video width=100% height=100% muted autoplay loop playsinline>
                  <source src="images/xnav_thumbnail_edited.mov" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <papertitle>X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots</papertitle>
            <br>
            <a href="https://wang-haitong.github.io">Haitong Wang</a>,
            <strong>Aaron Hao Tan</strong>,
            <a href="https://angusfung.github.io/">Angus Fung</a>,
            <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
            <br>
            <em>IEEE Robotics and Automation Letters 2025 + ICRA 2026</em>
            <br>
            <a href="https://arxiv.org/abs/2507.14731">Paper</a>
            /
            <a href="https://cross-embodiment-nav.github.io">Website</a>
            /
            <a href="https://www.youtube.com/watch?v=flDyg1Dbna0">Video</a>
            <p></p>
            <p>We introduce X-Nav, a novel framework for cross-embodiment navigation where a single unified policy can be deployed across various embodiments for both wheeled and quadrupedal robots.  </p>
          </td>
        </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sim2real_image'>
                  <video width=100% height=100% muted autoplay loop playsinline>
                    <source src="images/4cnet-outside.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>4CNet: A Diffusion Approach to Map Prediction for Decentralized Multi-Robot Exploration</papertitle>
              <br>
              <strong>Aaron Hao Tan</strong>,
              <a href="https://quest2gm.github.io">Siddarth Narasimhan</a>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>Under Review at T-RO, 2025</em>
              <br>
							<a href="https://arxiv.org/abs/2402.17904">Paper</a>
              /
							<a href="https://www.youtube.com/watch?v=nA2a3XXL5Dg">Video</a>
              <p></p>
              <p>We present a novel robot exploration map prediction method called Confidence-Aware Contrastive Conditional Consistency Model (4CNet), to predict (foresee) unknown spatial configurations in unknown unstructured multi- robot environments with irregularly shaped obstacles. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sim2real_image'>
                  <video width=100% height=100% muted autoplay loop playsinline>
                    <source src="images/mllm.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>MLLM-Search: A Zero-Shot Approach to Finding People using Multimodal Large Language Models</papertitle>
              <br>
              <a href="https://angusfung.github.io/">Angus Fung</a>,
              <strong>Aaron Hao Tan</strong>,
              <a href="https://wang-haitong.github.io">Haitong Wang</a>,
              <a href="https://scholar.google.ca/citations?user=ONlP52AAAAAJ&hl=en">Beno Benhabib</a>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>Robotics, 2025</em>
              <br>
							<a href="https://arxiv.org/abs/2412.00103">Paper</a>
              /
							<a href="https://www.youtube.com/watch?v=mzP3vcU611Y">Video</a>
              <p></p>
              <p>We present MLLM-Search, a novel multimodal language model approach to address the robotic person search problem under event-driven scenarios with incomplete or unavailable user schedules. Our method introduces zero-shot person search using language models for spatial reasoning. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sim2real_image'>
                  <video width=100% height=100% muted autoplay loop playsinline>
                    <source src="images/olivia-nav.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>OLiVia-Nav: An Online Lifelong Vision Language Approach for Mobile Robot Social Navigation</papertitle>
              <br>
              <a href="https://quest2gm.github.io">Siddarth Narasimhan</a>,
              <strong>Aaron Hao Tan</strong>,
              <a href="https://jeongwoongc.github.io/">Daniel Choi</a>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>ICRA 2025</em>
              <br>
              <em>CoRL Workshop: Lifelong Learning for Home Robots (<strong>Spotlight Presentation</strong>), 2024</em>
              <br>
							<a href="https://www.arxiv.org/abs/2409.13675">Paper</a>
              /
              <a href="files/CoRL_Poster_OLiVia-Nav_Final_v2.pdf">Poster</a>
              /
							<a href="https://www.youtube.com/watch?v=eyFJiOIITO0">Video</a>
              /
              <a href="https://youtu.be/Vlp_Pj_x61Y">Talk</a>
              <p></p>
              <p>We introduce OLiVia-Nav, an online lifelong vision language architecture for mobile robot social navigation. By leveraging large vision-language models and a novel distillation process called SC-CLIP, OLiVia-Nav efficiently encodes social and environmental contexts, adapting to dynamic human environments. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sim2real_image'>
                  <video width=100% height=100% muted autoplay loop playsinline>
                    <source src="images/finder_thumb.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Find Everything: A General Vision Language Model Approach to Multi-Object Search</papertitle>
              <br>
              <a href="https://jeongwoongc.github.io/">Daniel Choi</a>,
              <a href="https://angusfung.github.io/">Angus Fung</a>,
              <a href="https://wang-haitong.github.io">Haitong Wang</a>,
              <strong>Aaron Hao Tan</strong>
              <br>
              <em>IROS 2025</em>
              <br>
              <em>CoRL Workshop: Language and Robot Learning 2024</em>
              <br>
							<a href="https://arxiv.org/abs/2410.00388">Paper</a>
              /
							<a href="https://find-all-my-things.github.io/">Website</a>
              /
              <a href="https://www.youtube.com/watch?v=o7Q9-uF8Is4">Video</a>
              /
              <a href="files/CoRL_Poster_FiNDER_print.pdf">Poster</a>
              <p></p>
              <p>We present Finder, a novel approach to the multi-object search problem that leverages vision language models to efficiently locate multiple objects in diverse unknown environments. Our method combines semantic mapping with spatio-probabilistic reasoning and adaptive planning, improving object recognition and scene understanding through VLMs.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sim2real_image'>
                  <video width=100% height=100% muted autoplay loop playsinline>
                    <source src="images/navformer.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>NavFormer: A Transformer Architecture for Robot Target-Driven Navigation in Unknown and Dynamic Environments</papertitle>
              <br>
              <a href="https://wang-haitong.github.io">Haitong Wang</a>,
              <strong>Aaron Hao Tan</strong>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>IEEE Robotics and Automation Letters 2024 + ICRA 2025</em>
              <br>
							<a href="https://arxiv.org/abs/2402.06838">Paper</a>
              /
              <a href="https://www.youtube.com/watch?v=PSVsLM1eGXo">Video</a>
              <p></p>
              <p>We propose NavFormer, a novel end-to-end DL architecture consisting of a dual-visual encoder module and a transformer-based navigation network to address for the first time the problem of TDN in unknown and dynamic environments.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sim2real_image'>
                  <video width=100% height=100% muted autoplay loop playsinline>
                    <source src="images/made-net2.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Deep Reinforcement Learning for Decentralized Multi-Robot Exploration with Macro Actions</papertitle>
              <br>
              <strong>Aaron Hao Tan</strong>,
              <a href="https://www.linkedin.com/in/federico-pizarro-bejarano/?originalSubdomain=ca">Federico Pizzaro Bejarano</a>,
              <a href="">Yuhan Zhu</a>,
              <a href="">Richard Ren</a>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>IEEE Robotics and Automation Letters + ICRA 2023</em>
              <br>
							<a href="https://arxiv.org/abs/2110.02181">Paper</a>
              /
							<a href="https://youtu.be/iTzPRoOS3Q0">Video</a>
              /
              <a href="https://www.youtube.com/watch?v=pYceNa9IzfU&ab_channel=AutonomousSystemsandBiomechatronicsLab%28UniversityofToronto%29">Talk</a>
              /
							<a href="files/made-net.pdf">Poster</a>

              <p></p>
              <p>The first Macro Action Decentralized Exploration Network (MADE-Net) using multi-agent deep reinforcement learning to address the challenges of communication dropouts during multi-robot exploration in unseen, unstructured, and cluttered environments.</p>
            </td>
          </tr>

          <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='refnerf_image'>
                  <video width=100% height=100% muted autoplay loop playsinline>
                    <source src="images/Full Pillow Placement.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Development of a Pillow Placement Process for Robotic Bed-Making</papertitle>
                <br>
                <a href="https://www.linkedin.com/in/cheung-chihong/?originalSubdomain=ca">Chi-Hong Cheung</a>,
                <strong>Aaron Hao Tan</strong>,
                <a href="https://robotics.utoronto.ca/wp-content/uploads/sites/33/2024/01/Andrew-Goldenberg_profile_FINAL73.pdf">Andrew Goldenberg</a>
                <br>
                <em><a href="https://event.asme.org/IDETC-CIE">IEEE/ASME MESA</a></em>, 2023 &nbsp
                <br>
                <a href="https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings-abstract/IDETC-CIE2023/87356/1170626">Paper</a>
                <a href="files/pillow_paper.pdf"> (Access)</a>
                <p></p>
                <p>We introduce a robotic pillow placement system using a static 6-DOF manipulator, leveraging YOLOv4-tiny, image transformations, and PCA to infer pillow poses and execute macro-actions.</p>
              </td>
            </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sim2real_image'>
                  <video width=100% height=100% muted autoplay loop playsinline>
                    <source src="images/taxonomy.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Enhancing Robot Task Completion Through Environment and Task Inference: A Survey from the Mobile Robot Perspective</papertitle>
              <br>
              <strong>Aaron Hao Tan</strong>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
              <em>Journal of Intelligent and Robotic Systems</em>, 2022
              <br>
							<a href="https://link.springer.com/article/10.1007/s10846-022-01776-0">Paper</a>
              <p></p>
              <p>The first extensive investigation of mobile robot inference problems in unknown environments with limited sensor and communication range and propose a new taxonomy to classify the different environment and task inference methods for single- and multi-robot systems. </p>
            </td>
          </tr>

          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='sim2real_image'>
                <video width=100% height=100% muted autoplay loop playsinline>
                  <source src="images/sim2real.mov" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
          </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>A Sim-to-Real Pipeline for Deep Reinforcement Learning for Autonomous Robot Navigation in Cluttered Rough Terrain</papertitle>
              <br>
              <a href="https://rhklite.github.io/dist/index.html#publications">Han Hu</a>,
              <a href="https://www.linkedin.com/in/kczhang/?originalSubdomain=ca">Kaicheng Zhang</a>,
              <strong>Aaron Hao Tan</strong>,
              <a href="https://www.linkedin.com/in/m-ruan/?originalSubdomain=ca">Michael Ruan</a>,
              <a href="https://www.chrisagia.com/">Christopher Agia</a>,
              <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
              <br>
							<em>IEEE Robotics and Automation Letters + IROS</em>, 2021 &nbsp
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/9468918">Paper</a>
              /
              <a href="https://www.youtube.com/watch?v=GuE7cfknckg&ab_channel=AutonomousSystemsandBiomechatronicsLab%28UniversityofToronto%29">Talk</a>
              /
              <a href="https://www.youtube.com/watch?v=dtYlNWvK-7k&ab_channel=AutonomousSystemsandBiomechatronicsLab%28UniversityofToronto%29">Video</a>
              /
              <a href="https://www.youtube.com/watch?v=DjA-FNGtTs4&ab_channel=AaronTan">Baseline Demo</a>
              <p></p>
              <p>The development of a novel sim-to-real pipeline for a mobile robot to effectively learn how to navigate real-world 3D rough terrain environments.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='eight_image'>
                  <video width=100% height=100% muted autoplay loop playsinline>
                    <source src="images/8x8_loop.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Design and development of a novel autonomous scaled multiwheeled vehicle</papertitle>
                <br>
                <strong>Aaron Hao Tan</strong>,
                Michael Peiris,
                <a href="https://scholar.google.ca/citations?user=XOI7ib4AAAAJ&hl=en">Moustafa El-Gindy</a>,
                <a href="https://scholar.google.com/citations?user=GcttJ-oAAAAJ&hl=en">Haoxiang Lang</a> <br>
                <em>Robotica</em>, 2021 &nbsp <br>
                <em>ASME IDETC/CIE</em>, 2019 &nbsp
                <br>
                <a href="https://www.cambridge.org/core/journals/robotica/article/design-and-development-of-a-novel-autonomous-scaled-multiwheeled-vehicle/1029552C3A421608255F87C045E52FFA">Journal</a>
                <a href="files/design-and-development-of-a-novel-autonomous-scaled-multiwheeled-vehicle.pdf">(Access)</a>
                /
                <a href="https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings-abstract/IDETC-CIE2019/59216/1069918">Conference</a>
                <a href="files/IDETC2019-97163 FINAL.pdf">(Access)</a>
                /
                <a href="files/8x8 Presentation.pdf">Slides</a>
                /
                <a href="https://www.youtube.com/watch?v=42oM5fr4juA&ab_channel=AaronTan">Video</a>
                <p></p>
                <p>A 1:6 scale, multi-wheeled mobile robotic platform with independent suspension, steering and actuation for off-terrain operations.</p>
            </td>
          </tr>

				
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Robotics Projects</heading>
            </td>
          </tr>
        </tbody></table> -->


            <!-- <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()"> -->
            <!-- <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/navigation.mov" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                </div>

              </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Autonomous Navigation and Mapping</papertitle>
                  <br>
                  <strong>Aaron Hao Tan</strong>, 2018
                  <br>
                  <a href="https://www.youtube.com/watch?v=fM0ft-QHnnk&ab_channel=AaronTan">Indoor</a>/
                  <a href="https://youtu.be/wglpGi9582k">Outdoor</a>

                  <p></p>
                  <p>Implemented open source mapping, localization and navigation algorithms with Clearpath Husky.</p>
                </td>
              </tr> -->

          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Supplementary Works</heading>
                <p>
                  These are works that I have contributed to during my free time.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>


            <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/zinan_paper.jpg' width="160">
                </div>
              </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Piezoresistive Sensors Array for Multijoint Motion Estimation Application</papertitle>
                  <br>
                  <a href="https://www.linkedin.com/in/zinan-cen-2a2431116/?originalSubdomain=ca">Zinan Cen</a>,
                  <a href="https://www.linkedin.com/in/fraser-robinson/">Fraser Robinson</a>,
                  <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>,
                  <a href="https://scholar.google.ca/citations?user=GoATyK0AAAAJ&hl=en">Hani E. Naguib</a>
                  <br>
                  <strong>Aaron Hao Tan (Acknowledged)</strong>
                  <br>
                  <em>IEEE/ASME Transactions on Mechatronics</em>, 2024 &nbsp
                  <br>
                  <a href="files/Piezoresistive_Sensors_Array_for_Multijoint_Motion_Estimation_Application.pdf">Paper</a>
                  <p></p>
                  <p>I was acknowledged in the development of a smart clothing system using low-cost, energy-efficient strain sensors to measure elbow and shoulder joint angles. A novel neural network architecture was used to enhance the accuracy of the sensor signal mapping.</p>
                </td>
              </tr>

            <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/mask_detection.jpeg' width="160">
                </div>
              </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Robust Face Mask Detection by a Socially Assistive Robot Using Deep Learning</papertitle>
                  <br>
                  Yuan Zhang, Meysam Effati, 
                  <strong>Aaron Hao Tan</strong>,
                  <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
                  <br>
                  <em><a href="https://www.mdpi.com/2073-431X/13/1/7">Computers</a></em>, 2024 &nbsp
                  <br>
                  <a href="files/mask_detection_paper.pdf">Paper</a>
                  <p></p>
                  <p>We propose a novel two-step deep learning (DL) method based on our extended ResNet-50 model. It can detect and classify whether face masks are missing, are worn correctly or incorrectly, or the face is covered by other means (e.g., a hand or hair).</p>
                </td>
              </tr>

            <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='refnerf_image'>
                    <video width=100% height=100% muted autoplay loop playsinline>
                      <source src="images/Full Pillow Placement.mov" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                  </div>
                </div>
              </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Development of a Pillow Placement Process for Robotic Bed-Making</papertitle>
                  <br>
                  <a href="https://www.linkedin.com/in/cheung-chihong/?originalSubdomain=ca">Chi-Hong Cheung</a>,
                  <strong>Aaron Hao Tan</strong>,
                  <a href="https://robotics.utoronto.ca/wp-content/uploads/sites/33/2024/01/Andrew-Goldenberg_profile_FINAL73.pdf">Andrew Goldenberg</a>
                  <br>
                  <em><a href="https://event.asme.org/IDETC-CIE">IEEE/ASME MESA</a></em>, 2023 &nbsp
                  <br>
                  <a href="https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings-abstract/IDETC-CIE2023/87356/1170626">Paper</a>
                  <a href="files/pillow_paper.pdf"> (Access)</a>
                  <p></p>
                  <p>We propose a novel process for a 6-DOF static one-armed robotic manipulator to address the challenges of pillow placement on a small-scale bed.</p>
                </td>
              </tr> -->
        <!-- <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='eight_image'><video  width=100% height=100% muted autoplay loop>
              <source src="images/BS_figures.mov" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
            </div>
          </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>The Black Sheep Campaign</papertitle>
              <br>
              <strong>Aaron Hao Tan</strong>,
              <a href="https://angusfung.github.io/">Angus Fung</a>,
              <a href="https://sidedoormag.com/blog/2022/3/21/sean-leons-success-leading-up-to-being-featured-donda-2">Sean Leon</a>
              <br>
              <em>Currently in Development @ Pupil</em>, 2022-23 &nbsp
              <br>
              Websites: 
              <a href="https://www.herdimmunity.info/">Herd Immunity</a> /
              <a href="https://www.godsalgorithm.world/">God's Algorithm</a>
              <br>
              Photos: <a href="images/HI_billboard.jpeg">Billboard 1</a> / <a href="images/HI_billboard2.png">Billboard 2</a>
              <p></p>
              <p> Developing AI-based tools using SOTA text-to-image translation and natural language processing models to empower and facilitate the creative process at Pupil.
              </td>
          </tr> -->

        <!-- <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">

              <img src='images/hvf_input.png' width="160">
            </div>
          </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Deep Learning Model to Identify Homonymous Defects on Automated Perimetry</papertitle>
              <br>
              <strong>Aaron Hao Tan</strong>,
              <a href="https://www.researchgate.net/profile/Laura-Donaldson-3">Laura Donaldson</a>,
              <a href="">Luqmaan Moolla</a>,
              <a href="https://www.linkedin.com/in/austin-pereira-953199b9/">Austin Pereira</a>,
              <a href="https://www.prismeyeinstitute.com/about/our-doctors/dr-matt-schlenker/">Edward Margolin</a>
              <br>
							<em>British Journal of Ophthalmology</em>, 2022 &nbsp
              <br>
              <a href="https://bjo.bmj.com/content/early/2022/08/02/bjo-2021-320996">Paper</a> /
              <a href="files/NANOS2022AI.pdf">Slide <a href="https://www.nanosweb.org/i4a/pages/index.cfm?pageid=4229">(NANOS 2022)</a></a>
              <p></p>
              <p>The first application of deep learning to the classification of homonymous defects on automated perimetry.</p>
            </td>
          </tr>
          
          <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
                Your browser does not support the video tag.
                </video></div>
                <img src='images/jeeves.png' width="160">
              </div>
            </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Jeeves, the Ethically Designed Interface</papertitle>
                <br>
                <strong>Aaron Hao Tan</strong>,
                <a href="https://angusfung.github.io/">Angus Fung</a>,
                <a href="https://mphamhung.github.io/">Michael Pham-Hung</a>,
                <a href="hhttps://www.cristinagetson.com">Cristina Getson</a> <br>
                <em><a href="https://sites.google.com/view/ro-man-2021-r2d2/competition-results?authuser=0">RO-MAN: Roboethics Competition</a></em>, 2021 &nbsp
                <br>
                <a href="https://www.youtube.com/watch?v=1Y0t1s_2Aog&ab_channel=RAISELab">Talk</a>
                <p></p>
                <p>A competition designed to answer: how can robots respond and interact in an ethical manner when delivery objects within a domestic setting? Placed First.</p>
              </td>
            </tr> -->

<!-- 
            <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/trav_est_480.mov" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                </div>
              </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Mobile Robot Traversability Estimation with CNN</papertitle>
                  <br>
                  <strong>Aaron Hao Tan</strong>, 2019 &nbsp
                  <br>
                  <a href="https://www.youtube.com/watch?v=DjA-FNGtTs4&ab_channel=AaronTan">Video</a>
                  <p></p>
                  <p>Reproduced results from <a href="https://ieeexplore.ieee.org/abstract/document/8280544">Learning Ground Traversability from Simulations</a> on a custom environment.</p>
                </td>
              </tr> -->
  
          <!-- </tbody></table>


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='refnerf_image'><video width=100% height=100% muted autoplay loop playsinline>
                    <source src="images/vs.mov" type="video/mp4">
                    Your browser does not support the video tag.
                    </video></div>
                  </div>
                </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                    <papertitle>Image and Position based Visual Servoing (Mobile Robot and Manipulator)</papertitle>
                    <br>
                    <strong>Aaron Hao Tan</strong>
                    <br>
                    <em>ASME IDETC/CIE</em>, 2018 &nbsp
                    <br>
                    <em>IEEE CIVEMSA </em>, 2018 &nbsp
                    <br>
                    Papers: 
                    <a href="https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings-abstract/IDETC-CIE2018/V05AT07A078/275147">Image Based</a>
                    /
                    <a href="https://ieeexplore.ieee.org/abstract/document/8439978">Position Based</a>
                    / 
                    <a href="https://www.actapress.com/PaperInfo.aspx?PaperID=55328&reason=500">Docking</a>
                    <br>
                    Videos: 
                    <a href="https://www.youtube.com/watch?v=dzXyfbeat6g&ab_channel=AaronTan">UR5</a>
                    /
                    <a href="https://www.youtube.com/watch?v=x6bPpbCUvog&ab_channel=AaronTan">Docking</a>
                    <p></p>
                    <p>Implemented image and position-based visual servoing on a Clearpath Husky and UR5 Manipulator.</p>
                  </td>
                </tr> -->


        <!-- <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/iol.png' width="160">
            </div>
          </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>A Deep Learning Model to Predict Postoperative Refraction in Cataract Surgery</papertitle>
              <br>
              <strong>Aaron Hao Tan</strong>,
              <a href="">Luqmaan Moolla</a>,
              <a href="https://www.linkedin.com/in/austin-pereira-953199b9/">Austin Pereira</a>,
              <a href="https://www.prismeyeinstitute.com/about/our-doctors/dr-matt-schlenker/">Matthew Schlenker</a>,
              2021
              <br>
              <a href="files/IOL-AI-Manuscript-Figures.pdf">Report</a> /
              <a href="files/Toronto-AI-IOL-Model-Presentation-final.pdf">Slide</a>
              <p></p>
              <p>Our model demonstrated a higher accuracy and precision than both a modern IOL power formula (Barrett Universal II) and two classical ML approaches in predicting postoperative refraction in our data set of 2490 eyes. </p>
            </td>
          </tr> -->


          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/mdp.png' width="160">
              </div>
            </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Dynamic and Distributed Decision Making</papertitle>
                <br>
                <strong>Aaron Hao Tan</strong>, 2020 &nbsp
                <br>
                <a href="https://github.com/aarontan-git/MDP">MDP</a>
                /
                <a href="https://github.com/aarontan-git/RL_algorithms">RL</a>
                /
                <a href="https://github.com/aarontan-git/multi_agent_learning">MARL</a>
                <p></p>
                <p>Implemented value/policy iteration, Monte Carlo, QLearning, SARSA, TD-Lambda, Shapley's Value Iteration and Minimax QLearning</p>
              </td>
            </tr> -->


            <!-- <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/ds.png' width="160">
                </div>
              </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Deep Learning, Data Science and Analytics</papertitle>
                  <br>
                  <strong>Aaron Hao Tan</strong>, 2018-2019 &nbsp
                  <br>
                  <a href="https://github.com/aarontan-git/salary_classification">Salary Classification</a>
                  /
                  <a href="https://github.com/aarontan-git/sentiment_analysis">Sentiment Analysis</a>
                  <br>
                  <a href="https://github.com/aarontan-git/news_Busters">Fake News Classification</a>: <a href="files/Fake-News-Detection-Final.pdf">Report</a> / <a href="files/Fake-News-Detector-Slide.pdf">Slide</a>
                  <br>
                  <a href="files/LSTM_mini_lecture.pdf">LSTM Mini Lecture</a>
                  <br>
                  <a href="files/Deep Learning Mini Lecture CLASS.pdf">Deep Learning Mini Lecture</a>


                  <p></p>
                  <p>Completed several projects pertaining to deep learning, data science and analytics.</p>
                </td>
              </tr> -->

                <!-- <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                      <div class="two" id='eight_image'><video  width=100% height=100% muted autoplay loop>
                      <source src="images/ebike.mov" type="video/mp4">
                      Your browser does not support the video tag.
                      </video></div>
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                      <papertitle>Rider sweat prediction for pedal-assist mode</papertitle>
                      <br>
                      <strong>Aaron Hao Tan</strong>, 2017 &nbsp
                      <br>
                      <a href="https://www.youtube.com/watch?v=WCD9Q_Y4WIA&ab_channel=PedelecsundE-Bikes">ARIV E-Bike</a>
                      <p></p>
                      <p>Internship project at <a href="https://www.gm.ca/en/home.html"> General Motors </a> on a pedal-assisted power mode based on rider sweat onset prediction using machine learning, to enable rider to arrive without breaking a sweat.</p>
                  </td>
                </tr> -->

          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Recognition/Scholarships/Grants/Awards</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <td style="padding:20px;width:75%;vertical-align:middle">
            <strong>2024</strong>: <a href="images/stanford prism.png">PRISM Corhort - Stanford University</a> <br>
            <strong>2024</strong>: <a href="images/DCA.png">Doctoral Completion Award - University of Toronto ($4k)</a> <br>
            <strong>2024</strong>: <a href="images/ogs_2024.png">Ontario Graduate Scholarship - University of Toronto ($15k)</a> <br>
            <strong>2024</strong>: <a href="https://www.corl.org">Conference on Robot Learning </a> <a href="images/corl_review.png">(Reviewer Invitation) </a> <br>
            <strong>2024</strong>: <a href="https://offlocalhost.xyz/">LocalHost Fellowship </a> <a href="images/local_host.png">(Acceptance) </a> <br>
            <strong>2024</strong>: <a href="https://tatp.utoronto.ca/ta-awards/">Teaching Excellence Award </a> <a href="images/TATP Nominee.png">(Nominated) </a> <br>
            <strong>2023</strong>: <a href="images/microsoft_startup.jpeg">Microsoft Startup Hub Program ($150k)</a> <br>
            <strong>2023</strong>: <a href="images/OGS_2023.png">Ontario Graduate Scholarship - University of Toronto ($15k)</a> <br>
            <strong>2022</strong>: <a href="images/dunbar_description.png">William Dunbar Memorial Scholarship - University of Toronto ($6k) </a> <br>
            <strong>2022</strong>: <a href="images/apple scholar.png">Apple AI/ML Scholar Nominee - University of Toronto </a> <a href="images/apple scholar nominee.png">(1 of 3 selected across the entire university) </a> <br>
            <strong>2022</strong>: <a href="images/OGS 2022.png">Ontario Graduate Scholarship - University of Toronto ($15k)</a> <br>
            <strong>2022</strong>: <a href="https://www.mie.utoronto.ca/outstanding-teaching-assistants-recognized-by-mie/">MIE Teaching Assistant Award - University of Toronto ($500)</a> <br>
            <strong>2022</strong>: <a href="https://www.utoronto.ca/news/u-t-sets-stage-strategic-research-partnership-world-s-largest-ride-hailing-company">DiDi </a> <a href="images/DIDI 2022.png">Graduate Awards - University of Toronto ($10k)</a> <br>
            <strong>2022</strong>: <a href="https://sites.grenadine.co/sites/cos-sco/en/2022-cos-annual-meeting-and-exhibition/pages/25228">COS: Awards of Excellence in Ophthalmic Research - 2nd Place Collaborator</a><br>
            <strong>2021</strong>: <a href="https://sites.google.com/view/ro-man-2021-r2d2/competition-results?authuser=0">IEEE RO-MAN: The Roboethics Competition, McGill University - 1st Place ($1k)</a> <br>
            <strong>2021</strong>: <a href="files/IROS 2021 Chair Certificate.pdf">IROS: Outstanding Service as Chair of Technical Session</a> <br>
            <strong>2020</strong>: <a href="https://en.wikipedia.org/wiki/COVID-19">The world shut down ... </a> <br>
            <strong>2019</strong>: <a href="http://grasplab.ca/people.html">Outstanding Thesis Award Nomination - Ontario Tech University (MASc)</a> <br>
            <strong>2018</strong>: <a href="images/feas_grad.png" alt="Image description" target="_blank"> FEAS Graduate Scholarship - Ontario Tech University ($5k)</a> <br>
            <strong>2018</strong>: <a href="https://www.youtube.com/watch?v=6w2wrQ15SMc&ab_channel=KBS%EA%B5%90%EC%96%91" alt="Image description" target="_blank"> Appeared in an international documentary commisssioned by Korean Broadcasting System</a> <br>
            <strong>2017</strong>: <a href="https://www.youtube.com/watch?v=2ztyL68VKY8&ab_channel=BluePhoenixStudios" alt="Image description" target="_blank"> Capstone project video commissioned by Ontario Tech University for future students</a> <br>
            <strong>2017</strong>: <a href="images/first_place.jpg" alt="Image description" target="_blank"> 1st Place Senior Engineering Design Competition - Ontario Tech University ($500)</a> <br>
            <strong>2016</strong>: <a href="images/gm_assembly_plant.png" alt="Image description" target="_blank"> General Motors Assembly Plant Award ($2.5k)</a> <br>
            <strong>2016</strong>: <a href="https://research.ontariotechu.ca/students/nserc-usra.php">NSERC Undergraduate Student Research Awards ($8.6k)</a> <br>
            <strong>2012-2017</strong>: <a href="https://usgc.ontariotechu.ca/policy/policy-library/policies/academic/grading-system-and-academic-standing-policy-undergraduate.php#:~:text=Students%20in%20clear%20standing%20with%20a%20semester%20GPA%20of%203.8,President's%20List%20on%20their%20Transcripts.">President's List</a> <br>

          </td> -->
          
          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Teachings</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <td style="padding:20px;width:75%;vertical-align:middle">
            <strong>2024 F</strong>: MIE1517: Introduction to Deep Learning Tutorial TA, University of Toronto <br>
            <strong>2024 F</strong>: ROB301: Introduction to Robotics Tutorial TA, University of Toronto <br>
            <strong>2024 S</strong>: MIE1070: Intelligent Robots for Society Head TA, University of Toronto <br>
            <strong>2024 W</strong>: MIE443: Mechatronics Systems: Design & Integration Head TA, University of Toronto <br>
            <strong>2023 F</strong>: ROB301: Introduction to Robotics Tutorial TA, University of Toronto <br>
            <strong>2023 S</strong>: MIE1070: Intelligent Robots for Society Head TA, University of Toronto <br>
            <strong>2023 W</strong>: MIE443: Mechatronics Systems: Design & Integration Tutorial TA, University of Toronto <br>
            <strong>2022 F</strong>: ECE1724: Bio-inspired Algorithms for Smart Mobility, University of Toronto <br>
            <strong>2022 F</strong>: ROB301: Introduction to Robotics Tutorial TA, University of Toronto <br>
            <strong>2022 S</strong>: MIE1070: Intelligent Robots for Society Head TA, University of Toronto <br>
            <strong>2022 W</strong>: MIE443: Mechatronics Systems: Design & Integration Tutorial TA, University of Toronto <br>
            <strong>2022 W</strong>: ENH610: Parasitology and Pest Control Lab TA, Toronto Metropolitan University <br>
            <strong>2021 W</strong>: MIE443: Mechatronics Systems: Design & Integration Tutorial TA, University of Toronto <br>
            <strong>2020 W</strong>: MIE443: Mechatronics Systems: Design & Integration Lab TA, University of Toronto <br>
            <strong>2019 W</strong>: MECE3390U: Mechatronics Head TA, Ontario Tech University <br>
            <strong>2018 F</strong>: MECE2230U: Statics Head TA, Ontario Tech University <br>
            <strong>2018 W</strong>: MECE3390U: Mechatronics Head TA, Ontario Tech University  <br>
            <strong>2017 F</strong>: MECE3350U Control Systems Head TA, Ontario Tech University  <br>
            <br>
            <strong>Course Evaluations: </strong> University of Toronto (<a href="files/myTAEvaluation 2021-2022.pdf">Eval</a>/<a href="files/MIE teaching award certificate.pdf">Certificate</a>), Ontario Tech University (<a href="files/MECE3350U-2017F.pdf">Eval 1</a>/<a href="files/MECE3390U-2018W.PDF">Eval 2</a>) <br>
          

          </td> -->

          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody> 
            <tr>
              <td>
                <heading>Mentoring</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <td style="padding:20px;width:75%;vertical-align:middle">
            <strong>2024-2025</strong>: Master of Engineering Student: <a href="https://www.linkedin.com/in/sourabh-satheesh-prasad/">Sourabh Prasad</a>, Currently working on Cross Embodiment Navigation<br>
            <strong>2023-2024</strong>: Undergraduate Thesis Student: <a href="https://jeongwoongc.github.io/">Daniel Choi </a>  <a href="files/Daniel_thesis.pdf">(Thesis)</a>, Currently MASc Student at MIE UofT<br>
            <strong>2022-2023</strong>: Master of Engineering Student: <a href="https://www.linkedin.com/in/yuhan-zhu-19b50617b/">Yuhan Zhu </a>, Currently PhD Student at University of California, Riverside<br>
            <strong>2022-2023</strong>: Master of Engineering Student: <a href="https://scholar.google.ca/citations?user=LA6TYrgAAAAJ&hl=en&authuser=1">Haitong Wang </a>, Currently PhD Student at UofT<br>
            <strong>2022-2023</strong>: Undergraduate Thesis Student: Yuntao Cai, <a href="files/Thesis_Final_Report_Yuntao_Cai.pdf">(Thesis)</a>, Currently MASc Student at ECE UofT <br>
            <strong>2022-2023</strong>: Undergraduate Thesis Student: <a href="https://quest2gm.github.io">Siddarth Narasimhan </a> <a href="files/ESC499_Thesis_Final_Report_Siddarth.pdf">(Thesis)</a>, Currently MASc Student at MIE UofT<br>
            <strong>2021-2023</strong>: Master of Applied Science Student: <a href="https://www.linkedin.com/in/fraser-robinson/?originalSubdomain=ca">Fraser Robinson </a> <a href="files/Robinson_Fraser_202311_MAS_thesis.pdf">(Thesis)</a>, Currently at Revolve Surgical (YC S21)<br>
            <strong>2021-2022</strong>: Undergraduate Thesis Student: <a href="https://www.linkedin.com/in/yuhan-zhu-19b50617b/">Yuhan Zhu </a><a href="files/Yuhan Thesis Report.pdf">(Thesis)</a>, Currently PhD Student at University of California, Riverside<br>
            <strong>2021-2022</strong>: Undergraduate Thesis Student: Richard Ren <a href="files/Richard R Thesis Report.pdf">(Thesis)</a>, Currently Software Engineer at Amazon <br>
            <strong>2021-2022</strong>: Undergraduate Thesis Student: <a href="https://www.linkedin.com/in/giroele/?originalSubdomain=ca">Giro Ele </a><a href="files/Giro Ele Thesis Report.pdf">(Thesis)</a> <br>
            <strong>2020-2021</strong>: Undergraduate Thesis Student: <a href="https://www.linkedin.com/in/federico-pizarro-bejarano/">Federico Pizzaro Bejarano </a> <a href="files/Federico Thesis Report.pdf">(Thesis)</a>, Currently PhD Student at UofT <br>
            <strong>2020-2021</strong>: Undergraduate Thesis Student: <a href="https://www.linkedin.com/in/ge-lin/?trk=public_profile_browsemap&original_referer=https%3A%2F%2Fwww%2Egoogle%2Ecom%2F&originalSubdomain=ca">Ge Lin </a> <a href="files/Ge Lin Thesis Report.pdf">(Thesis) </a>, Currently M.Eng at McGill University <br>
          </td>           -->
          
          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Academic Services</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          
          <td style="padding:20px;width:75%;vertical-align:middle">
            <strong>2020-Present</strong>: Reviewer for <a href="https://www.ieee-ras.org/publications/ra-l">RA-L</a>, <a href="https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra">ICRA</a>, <a href="https://www.corl.org">CoRL</a>, <a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros">IROS</a>, <a href="https://roboticsconference.org/">RSS</a>, <a href="https://link.springer.com/journal/10846">JIRS</a>, <a href="https://link.springer.com/journal/10994">Machine Learning</a>, <a href="https://www.nature.com/srep/">Scientific Reports</a><br>
          </td>       -->

          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Updates</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <strong>2024 F</strong>: Spotlight Presentation @ <a href="https://www.corl.org/home">CoRL 2024</a> <a href="images/corl-spotlight.png">(Invite</a>, <a href="https://youtu.be/Vlp_Pj_x61Y"> Talk</a>)<br>
            <strong>2024 S</strong>: Keynote Speaker @ <a href="https://www.buildingtransformations.org/events/contech24">The Future of Construction</a> <a href="images/keynote_speaker.jpg">(Picture</a> <a href="https://www.youtube.com/watch?v=CZcanG8L3Bs">/Talk)</a> <br>
            <strong>2024 S</strong>: Interview @ <a href="https://www.ycombinator.com/">YCombinator</a> w/ <a href="https://angusfung.github.io/">Angus Fung </a> <a href="images/YC Syncere interview.png">(Invite 1)</a><br>
            <strong>2023 F</strong>: Final Round Onsite Interview @ <a href="https://www.eranyc.com/">ERA </a> w/ <a href="https://angusfung.github.io/">Angus Fung </a> <a href="images/ERA_interview.jpeg">(Interview Day)</a><br>
            <strong>2023 S</strong>: Placed 3rd in the prestigious Shot on iPhone Photography Award hosted by <a href="https://www.ippawards.com/2023-winning-photographers/?v=3e8d115eb4b3">IPPAWARDS <a href="https://www.ippawards.com/2023-winners-cityscape/?v=3e8d115eb4b3">(Picture)</a> <br>
            <strong>2023 W</strong>: Interview @ <a href="https://www.ycombinator.com/">YCombinator</a> w/ <a href="https://angusfung.github.io/">Angus Fung </a> <a href="images/YC interview 1.png">(Invite 1)</a>, <a href="images/YC interview 2.png">(Invite 2)</a><br>
            <strong>2023 W</strong>: Won 1st place in Toronto's competitive men's basketball league <a href="images/2023-W-champ.jpeg">(Picture)</a><br>
            <strong>2022 F</strong>: Won 1st place in Toronto's competitive men's basketball league <a href="images/2022-F-champ.png">(Picture)</a><br>
            <strong>2022 F</strong>: Designed billboards using DALL-E for Grammy Award recipient Sean Leon <a href="images/HI_billboard.jpeg">(Pic1</a>/ <a href="images/HI_billboard2.png">Pic2)</a><br>
            <strong>2022 S</strong>: Organized paintball social event for Robotics Institute at the University of Toronto <a href="images/Robotics Institute Paintball.jpeg">(Picture)</a><br>
            <strong>2022 S</strong>: Won 1st place in Toronto's competitive men's basketball league <a href="images/basketball team.jpeg">(Pic 1</a>/<a href="images/Jam_firstplace.jpg">Pic 2)</a> <br>
            <strong>2021 F</strong>: Won 2nd place in Toronto's recreational men's basketball league <a href="images/2021 basketball team.jpeg">(Picture)</a> <br>
            <strong>2019 S</strong>: Photographer and member of the Local Organization Committee for the <a href="http://www.ieee-iccse.org/2019/index.html">14th IEEE ICCSE Conference</a><br>
            <strong>2018 S</strong>: Featured in Apple's Shot on iPhone photography campaign <a href="images/Shotoniphone 1.jpeg">(Pic 1</a>/<a href="images/Shotoniphone 2.jpeg">Pic 2)</a> <br> <br>
          </td> -->

          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Projects</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>              

            <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/scholarply_logo.png' width="160">
                </div>
              </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Scholarply</papertitle>
                  <br>
                  <strong>Aaron Hao Tan</strong>,
                  <a href="https://angusfung.github.io/">Angus Fung</a>
                  <br>
                  2023 Q3-4 &nbsp
                  <br>
                  <a href="https://scholarply.com/">Scholarply</a> / <a href="https://cryptorank.io/news/feed/1727c-scholarships-applying-with-ai-tool">Article</a> / <a href="images/scholarply_newsletter.jpeg">Newsletter</a> / <a href="https://www.tiktok.com/@the.varsity/video/7306702650840583430?is_from_webapp=1&sender_device=pc&web_id=7322966712234640901">TikTok</a>
                  <br>
                  <a href="https://youtu.be/vcr5VihgeBQ">Demo</a>
                  <br>

                  <ul>
                    <li>Accelerating the scholarship application process via LLM agents to help students secure funding while focusing on their studies.</li>
                  </ul>
                  <ul>
                    <li>Selected by Microsoft Startup Hub Program, receiving grants worth $150k.</li>
                  </ul>
                  </td>
              </tr>            

            <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/one800_logo.png' width="160">
                </div>
              </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>ONE800</papertitle>
                  <br>
                  <strong>Aaron Hao Tan</strong>,
                  <a href="https://angusfung.github.io/">Angus Fung</a>
                  <br>
                  2023 Q1-2 &nbsp
                  <br>
                  <a href="http://3.230.3.191">ONE800</a> / <a href="https://twitter.com/one800chat">Twitter</a> / <a href="https://www.instagram.com/one800chat/">Instagram</a>
                  <br>
                  <a href="https://youtu.be/Eq4MWiPGJ5s?si=Ps890MJe8P8nZaSv">Demo</a> / Team (<a href="images/one800_team_dinner.png">Pic1</a> / <a href="images/one800_team_dinner2.png">Pic2</a>)
                  <br>
                  <ul>
                    <li>An all-in-one service built in to iMessage aimed at lowering the barrier of entry for LLMs and Generative AI.</li>
                  </ul>
                  <p> 
                  </td>
              </tr> -->

          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading>Shot on iPhone</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <td style="padding:20px;width:75%;vertical-align:middle">
              Currently in development ... for now checkout my photography portfolio on my <a href="https://www.instagram.com/aaronistan/?hl=en">IG</a>
            </td>
        </tbody></table> -->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>

            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

  <!-- ClusterMaps widget -->
  <div id="clustrmaps-widget-container">
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=fqBGFtRk6Ib7Xamd1yfzHOL61m0VmhFXiL6RFAs2ACY&cl=ffffff&w=a"></script>
  </div>
  

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
  </tbody></table>
  <table width="100%" align="center" border="0" cellpadding="20"><tbody>
    <td style="padding:20px;width:75%;vertical-align:middle;text-align:center">
      8/24 Forever
    </td>
</tbody></table>




</body>

</html>
